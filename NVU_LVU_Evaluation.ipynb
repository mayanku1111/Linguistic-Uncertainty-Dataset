{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c67c6002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "\n",
    "import sys\n",
    "PROJECT_ROOT = \".\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from llm_linguistic_confidence_study.models.openrouter_llama import generate_openrouter_llama\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1600e9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa_model:\n",
      "  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "  base_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "  save_path: null\n",
      "  lora_weight_path: null\n",
      "  temperature: 0.7\n",
      "  top_p: 0.95\n",
      "  top_k: 50\n",
      "  min_p: 0.0\n",
      "  max_tokens: 4096\n",
      "dataset:\n",
      "  defaults:\n",
      "  - grader_model: gpt-5-mini\n",
      "  name: mini_simple_qa\n",
      "  url: None\n",
      "  file_path: llm_linguistic_confidence_study/datasets/mini_simple_qa_test_set.csv\n",
      "  grader_model:\n",
      "    name: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "    base_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "    save_path: null\n",
      "    lora_weight_path: null\n",
      "    temperature: 0.7\n",
      "    top_p: 0.95\n",
      "    top_k: 50\n",
      "    min_p: 0.0\n",
      "    max_tokens: 4096\n",
      "metrics:\n",
      "  acc2:\n",
      "    _target_: metrics.Accuracy\n",
      "    name: acc\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: false\n",
      "  acc:\n",
      "    _target_: metrics.Accuracy\n",
      "    name: acc\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: true\n",
      "  ece2:\n",
      "    _target_: metrics.ECE\n",
      "    name: ece\n",
      "    n_bins: 15\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: false\n",
      "  ece:\n",
      "    _target_: metrics.ECE\n",
      "    name: ece\n",
      "    n_bins: 15\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: true\n",
      "  auroc2:\n",
      "    _target_: metrics.AUROC\n",
      "    name: auroc\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: false\n",
      "  auroc:\n",
      "    _target_: metrics.AUROC\n",
      "    name: auroc\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: true\n",
      "  attempted_count:\n",
      "    _target_: metrics.AttemptedCount\n",
      "    name: attempted_count\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: true\n",
      "pre_runned_batch:\n",
      "  qa_batch_id: null\n",
      "  grader_batch_id: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg_root = \"llm_linguistic_confidence_study/configs\"\n",
    "\n",
    "cfg = OmegaConf.create({\n",
    "  \"qa_model\": OmegaConf.load(f\"{cfg_root}/qa_model/Llama-3.1-8B-Instruct.yaml\"),\n",
    "  \"dataset\": OmegaConf.load(f\"{cfg_root}/dataset/mini_simple_qa.yaml\"),\n",
    "  \"metrics\": OmegaConf.load(f\"{cfg_root}/metrics/all.yaml\"),\n",
    "  \"pre_runned_batch\": OmegaConf.load(f\"{cfg_root}/pre_runned_batch/no_run.yaml\"),\n",
    "})\n",
    "cfg.dataset.grader_model = OmegaConf.load(f\"{cfg_root}/qa_model/Llama-3.1-8B-Instruct.yaml\")\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ceb191ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm_linguistic_confidence_study.datasets import SimpleQADataset\n",
    "# # If you need to use OpenRouter Llama, use generate_openrouter_llama as imported in CELL INDEX: 0\n",
    "\n",
    "# dataset_cfg: DictConfig = cfg.dataset\n",
    "# simple_qa_dataset = SimpleQADataset(dataset_cfg)\n",
    "# print(f\"Dataset: {simple_qa_dataset.name}, rows: {len(simple_qa_dataset.df)}\")\n",
    "# simple_qa_dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bc976ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Australia is Canberra.\n"
     ]
    }
   ],
   "source": [
    "from llm_linguistic_confidence_study.models.openrouter_llama import generate_openrouter_llama\n",
    "# Quick smoke test using the OPENROUTER_API_KEY variable defined in a later cell\n",
    "resp = generate_openrouter_llama(\"What is the capital of Australia?\", api_key=OPENROUTER_API_KEY, model_name=\"meta-llama/llama-3.1-8b-instruct\")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08dd14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenRouter API key and model name\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_MODEL = \"meta-llama/llama-3.1-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83cad26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaults:\n",
      "- grader_model: meta-llama/llama-3.1-8b-instruct\n",
      "name: simple_qa\n",
      "url: https://huggingface.co/datasets/basicv8vc/SimpleQA/blob/main/simple_qa_test_set.csv\n",
      "file_path: llm_linguistic_confidence_study/datasets/simple_qa_test_set.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "simple_qa_cfg = OmegaConf.create({\n",
    "    \"defaults\": [{\"grader_model\": OPENROUTER_MODEL}],\n",
    "    \"name\": \"simple_qa\",\n",
    "    \"url\": \"https://huggingface.co/datasets/basicv8vc/SimpleQA/blob/main/simple_qa_test_set.csv\",\n",
    "    \"file_path\": \"llm_linguistic_confidence_study/datasets/simple_qa_test_set.csv\"\n",
    "})\n",
    "\n",
    "cfg.dataset = simple_qa_cfg\n",
    "print(OmegaConf.to_yaml(cfg.dataset, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a73bf3cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[1;32m     94\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m SIMPLE_QA_EVAL_VANILLA_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(question\u001b[38;5;241m=\u001b[39mq)\n\u001b[0;32m---> 95\u001b[0m     resp \u001b[38;5;241m=\u001b[39m _call_openrouter(\n\u001b[1;32m     96\u001b[0m         prompt,\n\u001b[1;32m     97\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mOPENROUTER_API_KEY,\n\u001b[1;32m     98\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mOPENROUTER_MODEL,\n\u001b[1;32m     99\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mqa_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    100\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mqa_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    101\u001b[0m         top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mqa_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    102\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mqa_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m     answers\u001b[38;5;241m.\u001b[39mappend(resp)\n\u001b[1;32m    106\u001b[0m lvu_results \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: q, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLVU_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: a} \u001b[38;5;28;01mfor\u001b[39;00m q, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(questions, answers)]\n",
      "Cell \u001b[0;32mIn[70], line 88\u001b[0m, in \u001b[0;36m_call_openrouter\u001b[0;34m(prompt, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m allowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     87\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m allowed \u001b[38;5;129;01mand\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generate_openrouter_llama(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered)\n",
      "File \u001b[0;32m~/Documents/GitHub/Linguistic-Uncertainty-Dataset/llm_linguistic_confidence_study/models/openrouter_llama.py:21\u001b[0m, in \u001b[0;36mgenerate_openrouter_llama\u001b[0;34m(prompt, api_key, model_name, temperature, max_tokens)\u001b[0m\n\u001b[1;32m     16\u001b[0m api_key \u001b[38;5;241m=\u001b[39m api_key \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENROUTER_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://openrouter.ai/api/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m }\n\u001b[1;32m     22\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     25\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     26\u001b[0m     ]\n\u001b[1;32m     27\u001b[0m }\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# forward generation kwargs if provided\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_chunk_length()\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure dataset is loaded from cfg.dataset if simple_qa_df is not defined\n",
    "import os\n",
    "try:\n",
    "    simple_qa_df\n",
    "except NameError:\n",
    "    dataset_name = cfg.dataset.name\n",
    "    dataset_url = cfg.dataset.url\n",
    "    dataset_file_path = cfg.dataset.file_path\n",
    "    os.makedirs(os.path.dirname(dataset_file_path), exist_ok=True)\n",
    "    if not os.path.exists(dataset_file_path):\n",
    "        import requests as _rq\n",
    "        raw_url = str(dataset_url).replace(\"/blob/\", \"/resolve/\")\n",
    "        r = _rq.get(raw_url, allow_redirects=True, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with open(dataset_file_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    simple_qa_df = pd.read_csv(dataset_file_path)\n",
    "else:\n",
    "    dataset_name = getattr(cfg.dataset, \"name\", \"simple_qa\")\n",
    "\n",
    "# Normalize columns to ensure 'question' exists\n",
    "def _normalize_simple_qa_columns(df):\n",
    "    # Added 'problem' to question synonyms\n",
    "    q_syn = {'question', 'query', 'prompt', 'input', 'text', 'problem'}\n",
    "    a_syn = {'answer', 'response', 'output', 'target', 'label'}\n",
    "    lower = {c: c.lower().strip() for c in df.columns}\n",
    "    q_col = next((c for c in df.columns if lower[c] in q_syn), None)\n",
    "    a_col = next((c for c in df.columns if lower[c] in a_syn), None)\n",
    "    if q_col and q_col != 'question':\n",
    "        df.rename(columns={q_col: 'question'}, inplace=True)\n",
    "    if a_col and a_col != 'answer':\n",
    "        df.rename(columns={a_col: 'answer'}, inplace=True)\n",
    "    return df\n",
    "simple_qa_df = _normalize_simple_qa_columns(simple_qa_df)\n",
    "if 'question' not in simple_qa_df.columns:\n",
    "    raise KeyError(f\"'question' column missing after normalization. Columns: {list(simple_qa_df.columns)}\")\n",
    "\n",
    "# LVU extraction and decisiveness scoring using OpenRouter via generate_openrouter_llama\n",
    "from llm_linguistic_confidence_study.models.openrouter_llama import generate_openrouter_llama\n",
    "import re\n",
    "import inspect\n",
    "\n",
    "SIMPLE_QA_EVAL_VANILLA_TEMPLATE = \"\"\"\n",
    "Answer the following question using a succinct (at most one sentence) and full answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "DECISIVENESS_PROMPT = \"\"\"You will be given a question and a proposed answer. Your task is to extract any assertions made by the proposed answer; for each assertion, you should output a\n",
    "decisiveness score: a number in [0,1] representing how likely you think this assertion is to be true, judged solely based on the provided answer. You should pay\n",
    "special attention to the usage of any hedging modifiers in the original answer, used to convey uncertainty in the truthfulness of the assertion. If the proposed\n",
    "answer punts the question, the extracted assertion should be the empty string and the decisiveness score should be 1.0.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Question: criminal minds episode where jj becomes a profiler\n",
    "Proposed answer: I think JJ officially becomes a profiler in Criminal Minds season 6, episode 2.\n",
    "Extracted assertion: JJ officially becomes a profiler in Criminal Minds season 6, episode 2.\n",
    "Decisiveness score: 0.8.\n",
    "\n",
    "Question: criminal minds episode where jj becomes a profiler\n",
    "Proposed answer: JJ officially becomes a profiler in Criminal Minds season 6, in either the first or second episode.\n",
    "Extracted assertion: JJ officially becomes a profiler in Criminal Minds season 6, episode 1.\n",
    "Decisiveness score: 0.5.\n",
    "Extracted assertion: JJ officially becomes a profiler in Criminal Minds season 6, episode 2.\n",
    "Decisiveness score: 0.5.\n",
    "\n",
    "Question: criminal minds episode where jj becomes a profiler\n",
    "Proposed answer: I'm not really sure about this, but I think the episode in which JJ officially becomes a profiler in Criminal Minds may be episode 2 in season 6.\n",
    "Extracted assertion: JJ officially becomes a profiler in Criminal Minds season 6, episode 2.\n",
    "Decisiveness score: 0.6.\n",
    "\n",
    "Question: criminal minds episode where jj becomes a profiler\n",
    "Proposed answer: I don't know which episode you're referring to.\n",
    "Extracted assertion:\n",
    "Decisiveness score: 1.0\n",
    "\n",
    "Question: {question}\n",
    "Proposed answer: {response}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Filter kwargs to only those supported by generate_openrouter_llama\n",
    "def _call_openrouter(prompt, **kwargs):\n",
    "    sig = inspect.signature(generate_openrouter_llama)\n",
    "    allowed = set(sig.parameters.keys())\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in allowed and v is not None}\n",
    "    return generate_openrouter_llama(prompt, **filtered)\n",
    "\n",
    "# Use the CSV-backed DataFrame loaded above\n",
    "questions = simple_qa_df['question'].dropna().astype(str).tolist()\n",
    "answers = []\n",
    "for q in questions:\n",
    "    prompt = SIMPLE_QA_EVAL_VANILLA_TEMPLATE.format(question=q)\n",
    "    resp = _call_openrouter(\n",
    "        prompt,\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "        model_name=OPENROUTER_MODEL,\n",
    "        temperature=getattr(cfg.qa_model, \"temperature\", None),\n",
    "        top_p=getattr(cfg.qa_model, \"top_p\", None),\n",
    "        top_k=getattr(cfg.qa_model, \"top_k\", None),\n",
    "        max_tokens=getattr(cfg.qa_model, \"max_tokens\", None),\n",
    "    )\n",
    "    answers.append(resp)\n",
    "\n",
    "lvu_results = [{\"question\": q, \"LVU_answer\": a} for q, a in zip(questions, answers)]\n",
    "\n",
    "# Now ask decisiveness prompt per LVU answer\n",
    "\n",
    "decisiveness_scores = []\n",
    "for item in lvu_results:\n",
    "    prompt = DECISIVENESS_PROMPT.format(question=item['question'], response=item['LVU_answer'])\n",
    "    resp = _call_openrouter(\n",
    "        prompt,\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "        model_name=OPENROUTER_MODEL,\n",
    "        temperature=getattr(cfg.qa_model, \"temperature\", None),\n",
    "        top_p=getattr(cfg.qa_model, \"top_p\", None),\n",
    "        top_k=getattr(cfg.qa_model, \"top_k\", None),\n",
    "        max_tokens=getattr(cfg.qa_model, \"max_tokens\", None),\n",
    "    )\n",
    "    if resp is None:\n",
    "        score = None\n",
    "    else:\n",
    "        m = re.search(r\"Decisiveness score:\\s*([0-1](?:\\.\\d+)?)\", resp)\n",
    "        score = float(m.group(1)) if m else None\n",
    "    decisiveness_scores.append(score)\n",
    "\n",
    "import pandas as pd\n",
    "lvu_df = pd.DataFrame(lvu_results)\n",
    "lvu_df['decisiveness_score'] = decisiveness_scores\n",
    "print(\"LVU responses shape:\", lvu_df.shape)\n",
    "lvu_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20207250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LVU results and decisiveness scores\n",
    "out_dir = os.path.join(PROJECT_ROOT, \"llm_linguistic_confidence_study\", \"results\", dataset_name, OPENROUTER_MODEL, \"NVU_LVU_Notebook\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "lvu_df.to_csv(os.path.join(out_dir, \"lvu_responses_openrouter.csv\"), index=False)\n",
    "print(\"Saved LVU responses:\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c61070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Use OpenRouter Llama-3.1-8b-instruct for inference\n",
    "# from llm_linguistic_confidence_study.models.openrouter_llama import generate_openrouter_llama\n",
    "\n",
    "# prompt = \"What is the capital of Australia?\"\n",
    "# api_key = os.getenv(\"OPENROUTER_API_KEY\")  # Make sure your API key is set\n",
    "# response = generate_openrouter_llama(prompt, api_key=api_key)\n",
    "# print(\"OpenRouter Llama-3.1-8b-instruct response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4620ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# API_KEY = \"\"\n",
    "\n",
    "# url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# data = {\n",
    "#     \"model\": \"meta-llama/llama-3.1-8b-instruct\",  # you can swap this with other available models\n",
    "#     \"messages\": [\n",
    "#         {\"role\": \"user\", \"content\": \"what do you think of future of universe\"}\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     print(\"✅ API Key works!\")\n",
    "#     print(\"Response:\", response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "# else:\n",
    "#     print(\"❌ Something went wrong:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00740c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm_linguistic_confidence_study.confidence_extraction_methods.verbal_numerical_confidence import VerbalNumericalConfidenceExtractor\n",
    "\n",
    "# vnc_cfg = OmegaConf.load(os.path.join(cfg_root, \"confidence_extractor\", \"verbal_numerical_confidence.yaml\"))\n",
    "\n",
    "# vnc_cfg.qa_template = vnc_cfg.get(\"qa_template\", \"vanilla\")\n",
    "\n",
    "# vnc_extractor = VerbalNumericalConfidenceExtractor(vnc_cfg, cfg.qa_model)\n",
    "\n",
    "# vnc_df = vnc_extractor(simple_qa_dataset, qa_batch_job_id=None, grader_batch_job_id=None)\n",
    "# print(\"NVU responses shape:\", vnc_df.shape)\n",
    "# vnc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset pipeline driven by cfg.dataset\n",
    "dataset_name = cfg.dataset.name\n",
    "dataset_url = cfg.dataset.url\n",
    "dataset_file_path = cfg.dataset.file_path\n",
    "\n",
    "# Ensure local file exists; if not, download from HF (blob -> resolve URL)\n",
    "os.makedirs(os.path.dirname(dataset_file_path), exist_ok=True)\n",
    "if not os.path.exists(dataset_file_path):\n",
    "    import requests as _rq\n",
    "    raw_url = str(dataset_url).replace(\"/blob/\", \"/resolve/\")\n",
    "    r = _rq.get(raw_url, allow_redirects=True, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    with open(dataset_file_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# Load CSV to DataFrame and normalize columns\n",
    "simple_qa_df = pd.read_csv(dataset_file_path)\n",
    "def _normalize_simple_qa_columns(df):\n",
    "    # Added 'problem' to question synonyms\n",
    "    q_syn = {'question', 'query', 'prompt', 'input', 'text', 'problem'}\n",
    "    a_syn = {'answer', 'response', 'output', 'target', 'label'}\n",
    "    lower = {c: c.lower().strip() for c in df.columns}\n",
    "    q_col = next((c for c in df.columns if lower[c] in q_syn), None)\n",
    "    a_col = next((c for c in df.columns if lower[c] in a_syn), None)\n",
    "    if q_col and q_col != 'question':\n",
    "        df.rename(columns={q_col: 'question'}, inplace=True)\n",
    "    if a_col and a_col != 'answer':\n",
    "        df.rename(columns={a_col: 'answer'}, inplace=True)\n",
    "    return df\n",
    "simple_qa_df = _normalize_simple_qa_columns(simple_qa_df)\n",
    "if 'question' not in simple_qa_df.columns:\n",
    "    raise ValueError(f\"Could not infer 'question' column in {dataset_file_path}. Columns: {list(simple_qa_df.columns)}\")\n",
    "print(f\"Loaded dataset '{dataset_name}' with {len(simple_qa_df)} rows from {dataset_file_path}; columns: {list(simple_qa_df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
