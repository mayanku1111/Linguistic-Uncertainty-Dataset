{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c67c6002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "\n",
    "import sys\n",
    "PROJECT_ROOT = \".\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from llm_linguistic_confidence_study.models.openrouter_llama import generate_openrouter_llama\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1600e9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa_model:\n",
      "  name: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "  base_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "  save_path: null\n",
      "  lora_weight_path: null\n",
      "  temperature: 0.7\n",
      "  top_p: 0.95\n",
      "  top_k: 50\n",
      "  min_p: 0.0\n",
      "  max_tokens: 4096\n",
      "dataset:\n",
      "  defaults:\n",
      "  - grader_model: gpt-5-mini\n",
      "  name: mini_simple_qa\n",
      "  url: None\n",
      "  file_path: llm_linguistic_confidence_study/datasets/mini_simple_qa_test_set.csv\n",
      "  grader_model:\n",
      "    name: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "    base_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "    save_path: null\n",
      "    lora_weight_path: null\n",
      "    temperature: 0.7\n",
      "    top_p: 0.95\n",
      "    top_k: 50\n",
      "    min_p: 0.0\n",
      "    max_tokens: 4096\n",
      "metrics:\n",
      "  acc2:\n",
      "    _target_: metrics.Accuracy\n",
      "    name: acc\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: false\n",
      "  acc:\n",
      "    _target_: metrics.Accuracy\n",
      "    name: acc\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: true\n",
      "  ece2:\n",
      "    _target_: metrics.ECE\n",
      "    name: ece\n",
      "    n_bins: 15\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: false\n",
      "  ece:\n",
      "    _target_: metrics.ECE\n",
      "    name: ece\n",
      "    n_bins: 15\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: true\n",
      "  auroc2:\n",
      "    _target_: metrics.AUROC\n",
      "    name: auroc\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: false\n",
      "  auroc:\n",
      "    _target_: metrics.AUROC\n",
      "    name: auroc\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: true\n",
      "  attempted_count:\n",
      "    _target_: metrics.AttemptedCount\n",
      "    name: attempted_count\n",
      "    format: simpleqa_like\n",
      "    exclude_not_attempted: true\n",
      "pre_runned_batch:\n",
      "  qa_batch_id: null\n",
      "  grader_batch_id: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg_root = \"llm_linguistic_confidence_study/configs\"\n",
    "\n",
    "cfg = OmegaConf.create({\n",
    "  \"qa_model\": OmegaConf.load(f\"{cfg_root}/qa_model/Llama-3.1-8B-Instruct.yaml\"),\n",
    "  \"dataset\": OmegaConf.load(f\"{cfg_root}/dataset/mini_simple_qa.yaml\"),\n",
    "  \"metrics\": OmegaConf.load(f\"{cfg_root}/metrics/all.yaml\"),\n",
    "  \"pre_runned_batch\": OmegaConf.load(f\"{cfg_root}/pre_runned_batch/no_run.yaml\"),\n",
    "})\n",
    "cfg.dataset.grader_model = OmegaConf.load(f\"{cfg_root}/qa_model/Llama-3.1-8B-Instruct.yaml\")\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ceb191ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm_linguistic_confidence_study.datasets import SimpleQADataset\n",
    "# # If you need to use OpenRouter Llama, use generate_openrouter_llama as imported in CELL INDEX: 0\n",
    "\n",
    "# dataset_cfg: DictConfig = cfg.dataset\n",
    "# simple_qa_dataset = SimpleQADataset(dataset_cfg)\n",
    "# print(f\"Dataset: {simple_qa_dataset.name}, rows: {len(simple_qa_dataset.df)}\")\n",
    "# simple_qa_dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bc976ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Australia is Canberra.\n"
     ]
    }
   ],
   "source": [
    "from llm_linguistic_confidence_study.models.openrouter_llama import generate_openrouter_llama\n",
    "# Quick smoke test using the OPENROUTER_API_KEY variable defined in a later cell\n",
    "resp = generate_openrouter_llama(\"What is the capital of Australia?\", api_key=OPENROUTER_API_KEY, model_name=\"meta-llama/llama-3.1-8b-instruct\")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08dd14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenRouter API key and model name\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_MODEL = \"meta-llama/llama-3.1-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83cad26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaults:\n",
      "- grader_model: meta-llama/llama-3.1-8b-instruct\n",
      "name: simple_qa\n",
      "url: https://huggingface.co/datasets/basicv8vc/SimpleQA/blob/main/simple_qa_test_set.csv\n",
      "file_path: llm_linguistic_confidence_study/datasets/simple_qa_test_set.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "simple_qa_cfg = OmegaConf.create({\n",
    "    \"defaults\": [{\"grader_model\": OPENROUTER_MODEL}],\n",
    "    \"name\": \"simple_qa\",\n",
    "    \"url\": \"https://huggingface.co/datasets/basicv8vc/SimpleQA/blob/main/simple_qa_test_set.csv\",\n",
    "    \"file_path\": \"llm_linguistic_confidence_study/datasets/simple_qa_test_set.csv\"\n",
    "})\n",
    "\n",
    "cfg.dataset = simple_qa_cfg\n",
    "print(OmegaConf.to_yaml(cfg.dataset, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a73bf3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using first 500 questions for evaluation.\n",
      "LVU responses shape: (500, 3)\n",
      "LVU responses shape: (500, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>LVU_answer</th>\n",
       "      <th>decisiveness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who received the IEEE Frank Rosenblatt Award i...</td>\n",
       "      <td>Eero Simoncelli received the IEEE Frank Rosenb...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was awarded the Oceanography Society's Jer...</td>\n",
       "      <td>I don't have the information on who was awarde...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What's the name of the women's liberal arts co...</td>\n",
       "      <td>The name of the women's liberal arts college i...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In whose honor was the Leipzig 1877 tournament...</td>\n",
       "      <td>The Leipzig 1877 tournament was organized in h...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to Karl Küchler, what did Empress El...</td>\n",
       "      <td>Karl Küchler stated that Empress Elizabeth of ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Who received the IEEE Frank Rosenblatt Award i...   \n",
       "1  Who was awarded the Oceanography Society's Jer...   \n",
       "2  What's the name of the women's liberal arts co...   \n",
       "3  In whose honor was the Leipzig 1877 tournament...   \n",
       "4  According to Karl Küchler, what did Empress El...   \n",
       "\n",
       "                                          LVU_answer  decisiveness_score  \n",
       "0  Eero Simoncelli received the IEEE Frank Rosenb...                 1.0  \n",
       "1  I don't have the information on who was awarde...                 0.0  \n",
       "2  The name of the women's liberal arts college i...                 1.0  \n",
       "3  The Leipzig 1877 tournament was organized in h...                 0.9  \n",
       "4  Karl Küchler stated that Empress Elizabeth of ...                 1.0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure dataset is loaded fresh from cfg.dataset (avoid stale DataFrame from earlier runs)\n",
    "import os\n",
    "dataset_name = cfg.dataset.name\n",
    "dataset_url = cfg.dataset.url\n",
    "dataset_file_path = cfg.dataset.file_path\n",
    "os.makedirs(os.path.dirname(dataset_file_path), exist_ok=True)\n",
    "if not os.path.exists(dataset_file_path):\n",
    "    import requests as _rq\n",
    "    raw_url = str(dataset_url).replace(\"/blob/\", \"/resolve/\")\n",
    "    r = _rq.get(raw_url, allow_redirects=True, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    with open(dataset_file_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "simple_qa_df = pd.read_csv(dataset_file_path)\n",
    "\n",
    "# Normalize columns to ensure 'question' exists\n",
    "def _normalize_simple_qa_columns(df):\n",
    "    # Added 'problem' to question synonyms\n",
    "    q_syn = {'question', 'query', 'prompt', 'input', 'text', 'problem'}\n",
    "    a_syn = {'answer', 'response', 'output', 'target', 'label'}\n",
    "    lower = {c: c.lower().strip() for c in df.columns}\n",
    "    q_col = next((c for c in df.columns if lower[c] in q_syn), None)\n",
    "    a_col = next((c for c in df.columns if lower[c] in a_syn), None)\n",
    "    if q_col and q_col != 'question':\n",
    "        df.rename(columns={q_col: 'question'}, inplace=True)\n",
    "    if a_col and a_col != 'answer':\n",
    "        df.rename(columns={a_col: 'answer'}, inplace=True)\n",
    "    return df\n",
    "simple_qa_df = _normalize_simple_qa_columns(simple_qa_df)\n",
    "if 'question' not in simple_qa_df.columns:\n",
    "    raise KeyError(f\"'question' column missing after normalization. Columns: {list(simple_qa_df.columns)}\")\n",
    "\n",
    "# Limit to first N questions\n",
    "MAX_Q = 500\n",
    "simple_qa_df = simple_qa_df.head(MAX_Q)\n",
    "print(f\"Using first {len(simple_qa_df)} questions for evaluation.\")\n",
    "\n",
    "# LVU extraction and decisiveness scoring using OpenRouter via generate_openrouter_llama\n",
    "from llm_linguistic_confidence_study.models.openrouter_llama import generate_openrouter_llama\n",
    "import re\n",
    "import inspect\n",
    "\n",
    "SIMPLE_QA_EVAL_VANILLA_UNCERTAINTY_TEMPLATE = \"\"\"\n",
    "Answer the following question using a succinct (at most one sentence) and full answer. If you are uncertain about your answer to the question, convey this uncertainty linguistically by precisely hedging this answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Replace decisiveness prompt with LVU judge confidence prompt\n",
    "DECISIVENESS_PROMPT = \"\"\"Prompt for the LVU judge\n",
    "You will be given a question and a proposed answer generated by a language model. Your\n",
    "task is to output an confidence score: a number in [0,1] representing how certain the language\n",
    "model is about the truthfulness of its generated answer, judged solely based on the provided\n",
    "answer. You should pay special attention to the usage of any hedging modifiers in the original\n",
    "answer, used to convey uncertainty in the truthfulness of the answer. A score close to 1.0\n",
    "indicates that the model is very certain about the truthfulness answer. And a score close to\n",
    "0.0 means that the model is highly uncertain, and the answer likely contains some hedging expressions. If the proposed answer punts the question, the confidence score should be 0.0.\n",
    "Here are some examples:\n",
    "Question: criminal minds episode where jj becomes a profiler\n",
    "Proposed answer: I think JJ officially becomes a profiler in Criminal Minds season 6,\n",
    "episode 2.\n",
    "Confidence score: 0.8\n",
    "Question: criminal minds episode where jj becomes a profiler\n",
    "Proposed answer: JJ officially becomes a profiler in Criminal Minds season 6, in either the\n",
    "first or second episode.\n",
    "Confidence score: 0.5\n",
    "Question: criminal minds episode where jj becomes a profiler\n",
    "Proposed answer: I’m not really sure about this, but I think the episode in which JJ officially\n",
    "becomes a profiler in Criminal Minds may be episode 2 in season 6.\n",
    "Confidence score: 0.6\n",
    "Question: criminal minds episode where jj becomes a profiler\n",
    "Proposed answer: I don’t know which episode you’re referring to.\n",
    "Confidence score: 0.0\n",
    "Here are the question and proposed answer to be judged:\n",
    "Question:\n",
    "{question}\n",
    "Proposed Answer:\n",
    "{response}\n",
    "Provide your response only in the following format:\n",
    "Confidence score: [confidence score (0-1)].\"\"\".strip()\n",
    "\n",
    "# Filter kwargs to only those supported by generate_openrouter_llama\n",
    "def _call_openrouter(prompt, **kwargs):\n",
    "    sig = inspect.signature(generate_openrouter_llama)\n",
    "    allowed = set(sig.parameters.keys())\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in allowed and v is not None}\n",
    "    return generate_openrouter_llama(prompt, **filtered)\n",
    "\n",
    "# Use the CSV-backed DataFrame loaded above\n",
    "questions = simple_qa_df['question'].dropna().astype(str).tolist()\n",
    "answers = []\n",
    "for q in questions:\n",
    "    prompt = SIMPLE_QA_EVAL_VANILLA_TEMPLATE.format(question=q)\n",
    "    resp = _call_openrouter(\n",
    "        prompt,\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "        model_name=OPENROUTER_MODEL,\n",
    "        # Answer generation params (concise, mildly deterministic)\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    answers.append(resp)\n",
    "\n",
    "lvu_results = [{\"question\": q, \"LVU_answer\": a} for q, a in zip(questions, answers)]\n",
    "\n",
    "# Now ask decisiveness prompt per LVU answer\n",
    "\n",
    "decisiveness_scores = []\n",
    "for item in lvu_results:\n",
    "    prompt = DECISIVENESS_PROMPT.format(question=item['question'], response=item['LVU_answer'])\n",
    "    resp = _call_openrouter(\n",
    "        prompt,\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "        model_name=OPENROUTER_MODEL,\n",
    "        # Judge params (deterministic parsing-friendly)\n",
    "        temperature=0.7,\n",
    "        top_p=1.0,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    if resp is None:\n",
    "        score = None\n",
    "    else:\n",
    "        # Match either 'Confidence score:' or legacy 'Decisiveness score:' (case-insensitive)\n",
    "        m = re.search(r\"(?i)(?:Confidence|Decisiveness)\\s*score:\\s*([0-1](?:\\.\\d+)?)\", resp)\n",
    "        score = float(m.group(1)) if m else None\n",
    "    decisiveness_scores.append(score)\n",
    "\n",
    "import pandas as pd\n",
    "lvu_df = pd.DataFrame(lvu_results)\n",
    "lvu_df['decisiveness_score'] = decisiveness_scores\n",
    "print(\"LVU responses shape:\", lvu_df.shape)\n",
    "lvu_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20207250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LVU responses: ./llm_linguistic_confidence_study/results/simple_qa/meta-llama/llama-3.1-8b-instruct/NVU_LVU_Notebook\n"
     ]
    }
   ],
   "source": [
    "# Save LVU results and decisiveness scores\n",
    "out_dir = os.path.join(PROJECT_ROOT, \"llm_linguistic_confidence_study\", \"results\", dataset_name, OPENROUTER_MODEL, \"NVU_LVU_Notebook\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "lvu_df.to_csv(os.path.join(out_dir, \"lvu_responses_openrouter.csv\"), index=False)\n",
    "print(\"Saved LVU responses:\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c61070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Use OpenRouter Llama-3.1-8b-instruct for inference\n",
    "# from llm_linguistic_confidence_study.models.openrouter_llama import generate_openrouter_llama\n",
    "\n",
    "# prompt = \"What is the capital of Australia?\"\n",
    "# api_key = os.getenv(\"OPENROUTER_API_KEY\")  # Make sure your API key is set\n",
    "# response = generate_openrouter_llama(prompt, api_key=api_key)\n",
    "# print(\"OpenRouter Llama-3.1-8b-instruct response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4620ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# API_KEY = \"\"\n",
    "\n",
    "# url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# data = {\n",
    "#     \"model\": \"meta-llama/llama-3.1-8b-instruct\",  # you can swap this with other available models\n",
    "#     \"messages\": [\n",
    "#         {\"role\": \"user\", \"content\": \"what do you think of future of universe\"}\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     print(\"✅ API Key works!\")\n",
    "#     print(\"Response:\", response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "# else:\n",
    "#     print(\"❌ Something went wrong:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00740c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm_linguistic_confidence_study.confidence_extraction_methods.verbal_numerical_confidence import VerbalNumericalConfidenceExtractor\n",
    "\n",
    "# vnc_cfg = OmegaConf.load(os.path.join(cfg_root, \"confidence_extractor\", \"verbal_numerical_confidence.yaml\"))\n",
    "\n",
    "# vnc_cfg.qa_template = vnc_cfg.get(\"qa_template\", \"vanilla\")\n",
    "\n",
    "# vnc_extractor = VerbalNumericalConfidenceExtractor(vnc_cfg, cfg.qa_model)\n",
    "\n",
    "# vnc_df = vnc_extractor(simple_qa_dataset, qa_batch_job_id=None, grader_batch_job_id=None)\n",
    "# print(\"NVU responses shape:\", vnc_df.shape)\n",
    "# vnc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset pipeline driven by cfg.dataset\n",
    "dataset_name = cfg.dataset.name\n",
    "dataset_url = cfg.dataset.url\n",
    "dataset_file_path = cfg.dataset.file_path\n",
    "\n",
    "# Ensure local file exists; if not, download from HF (blob -> resolve URL)\n",
    "os.makedirs(os.path.dirname(dataset_file_path), exist_ok=True)\n",
    "if not os.path.exists(dataset_file_path):\n",
    "    import requests as _rq\n",
    "    raw_url = str(dataset_url).replace(\"/blob/\", \"/resolve/\")\n",
    "    r = _rq.get(raw_url, allow_redirects=True, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    with open(dataset_file_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# Load CSV to DataFrame and normalize columns\n",
    "simple_qa_df = pd.read_csv(dataset_file_path)\n",
    "def _normalize_simple_qa_columns(df):\n",
    "    # Added 'problem' to question synonyms\n",
    "    q_syn = {'question', 'query', 'prompt', 'input', 'text', 'problem'}\n",
    "    a_syn = {'answer', 'response', 'output', 'target', 'label'}\n",
    "    lower = {c: c.lower().strip() for c in df.columns}\n",
    "    q_col = next((c for c in df.columns if lower[c] in q_syn), None)\n",
    "    a_col = next((c for c in df.columns if lower[c] in a_syn), None)\n",
    "    if q_col and q_col != 'question':\n",
    "        df.rename(columns={q_col: 'question'}, inplace=True)\n",
    "    if a_col and a_col != 'answer':\n",
    "        df.rename(columns={a_col: 'answer'}, inplace=True)\n",
    "    return df\n",
    "simple_qa_df = _normalize_simple_qa_columns(simple_qa_df)\n",
    "if 'question' not in simple_qa_df.columns:\n",
    "    raise ValueError(f\"Could not infer 'question' column in {dataset_file_path}. Columns: {list(simple_qa_df.columns)}\")\n",
    "\n",
    "# Limit to first N questions\n",
    "MAX_Q = 200\n",
    "simple_qa_df = simple_qa_df.head(MAX_Q)\n",
    "print(f\"Loaded dataset '{dataset_name}' with {len(simple_qa_df)} rows (limited to first {MAX_Q}) from {dataset_file_path}; columns: {list(simple_qa_df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
