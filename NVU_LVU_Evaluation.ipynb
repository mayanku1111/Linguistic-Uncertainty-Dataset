{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from omegaconf import OmegaConf\n",
        "from omegaconf import DictConfig\n",
        "\n",
        "\n",
        "import sys\n",
        "PROJECT_ROOT = \"/Users/mayankkumar/Documents/GitHub/Linguistic-Uncertainty-Dataset\"\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "\n",
        "from llm_linguistic_confidence_study.datasets import load_dataset\n",
        "from llm_linguistic_confidence_study.confidence_extraction_methods import ConfidenceExtractor\n",
        "from llm_linguistic_confidence_study.metrics import MetricEvaluator\n",
        "\n",
        "print(\"Project root:\", PROJECT_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "cfg_root = os.path.join(PROJECT_ROOT, \"llm_linguistic_confidence_study\", \"configs\")\n",
        "\n",
        "cfg = OmegaConf.create({\n",
        "    \"qa_model\": OmegaConf.load(os.path.join(cfg_root, \"qa_model\", \"gpt-5-mini.yaml\")),\n",
        "    \"dataset\": OmegaConf.load(os.path.join(cfg_root, \"dataset\", \"mini_simple_qa.yaml\")),\n",
        "    \"metrics\": OmegaConf.load(os.path.join(cfg_root, \"metrics\", \"all.yaml\")),\n",
        "    \"pre_runned_batch\": OmegaConf.load(os.path.join(cfg_root, \"pre_runned_batch\", \"no_run.yaml\")),\n",
        "})\n",
        "\n",
        "print(OmegaConf.to_yaml(cfg, resolve=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from llm_linguistic_confidence_study.datasets import SimpleQADataset\n",
        "\n",
        "dataset_cfg: DictConfig = cfg.dataset\n",
        "simple_qa_dataset = SimpleQADataset(dataset_cfg)\n",
        "print(f\"Dataset: {simple_qa_dataset.name}, rows: {len(simple_qa_dataset.df)}\")\n",
        "simple_qa_dataset.df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from llm_linguistic_confidence_study.confidence_extraction_methods.verbal_numerical_confidence import VerbalNumericalConfidenceExtractor\n",
        "\n",
        "vnc_cfg = OmegaConf.load(os.path.join(cfg_root, \"confidence_extractor\", \"verbal_numerical_confidence.yaml\"))\n",
        "\n",
        "vnc_cfg.qa_template = vnc_cfg.get(\"qa_template\", \"vanilla\")\n",
        "\n",
        "vnc_extractor = VerbalNumericalConfidenceExtractor(vnc_cfg, cfg.qa_model)\n",
        "\n",
        "vnc_df = vnc_extractor(simple_qa_dataset, qa_batch_job_id=None, grader_batch_job_id=None)\n",
        "print(\"NVU responses shape:\", vnc_df.shape)\n",
        "vnc_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from llm_linguistic_confidence_study.confidence_extraction_methods.linguistic_confidence import LinguisticConfidenceExtractor\n",
        "\n",
        "lvu_cfg = OmegaConf.load(os.path.join(cfg_root, \"confidence_extractor\", \"linguistic_confidence.yaml\"))\n",
        "\n",
        "lvu_extractor = LinguisticConfidenceExtractor(lvu_cfg, cfg.qa_model)\n",
        "\n",
        "lvu_df = lvu_extractor(simple_qa_dataset, qa_batch_job_id=None, grader_batch_job_id=None)\n",
        "print(\"LVU responses shape:\", lvu_df.shape)\n",
        "lvu_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from llm_linguistic_confidence_study.metrics import Accuracy, ECE, AUROC\n",
        "\n",
        "metrics_cfg = cfg.metrics\n",
        "\n",
        "metric_list = [\n",
        "    OmegaConf.create({\"name\": \"acc\", \"format\": \"simpleqa_like\", \"exclude_not_attempted\": True}),\n",
        "    OmegaConf.create({\"name\": \"ece\", \"format\": \"simpleqa_like\", \"exclude_not_attempted\": True, \"n_bins\": 15}),\n",
        "    OmegaConf.create({\"name\": \"auroc\", \"format\": \"simpleqa_like\", \"exclude_not_attempted\": True}),\n",
        "]\n",
        "\n",
        "results = []\n",
        "for name, df in [(\"NVU\", vnc_df), (\"LVU\", lvu_df)]:\n",
        "    for mc in metric_list:\n",
        "        evaluator = MetricEvaluator(mc, simple_qa_dataset)\n",
        "        score = evaluator.evaluate(df)\n",
        "        results.append({\"method\": name, \"metric\": mc.name, \"score\": score})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "out_dir = os.path.join(PROJECT_ROOT, \"llm_linguistic_confidence_study\", \"results\", simple_qa_dataset.name, cfg.qa_model.name, \"NVU_LVU_Notebook\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "vnc_df.to_csv(os.path.join(out_dir, \"nvu_responses.csv\"), index=False)\n",
        "lvu_df.to_csv(os.path.join(out_dir, \"lvu_responses.csv\"), index=False)\n",
        "results_df.to_csv(os.path.join(out_dir, \"metrics.csv\"), index=False)\n",
        "\n",
        "print(\"Saved:\", out_dir)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
